{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzXV1Zr4gamSnsRT5cvKkF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trilokgoel/Company_NER_S-P500_News/blob/main/MA_NER_spacy_base_optu_mltcls_workg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jFkN7MpbQCR",
        "outputId": "9b8ea51b-dcb2-4d3b-aee9-7bf135124a07"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EKEqPfukaFa1"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.util import filter_spans\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "import random\n",
        "from datetime import datetime\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcNlYOlyaPzR",
        "outputId": "a360299a-2b70-4a27-8c48-e907c6b231e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedSpacyNERModel:\n",
        "    \"\"\"\n",
        "    Enhanced Spacy NER model for M&A entity recognition with 4 classes and hyperparameter tuning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize the EnhancedSpacyNERModel.\n",
        "\n",
        "        Args:\n",
        "            model_name (str, optional): Name of existing Spacy model to load. Defaults to None (new model).\n",
        "        \"\"\"\n",
        "        self.nlp = spacy.blank(\"en\") if model_name is None else spacy.load(model_name)\n",
        "        self.entity_types = ['ACQUIRER', 'TARGET', 'SELLER', 'NOT_M&A']  # Our custom entity labels\n",
        "        self.best_params = None  # To store best hyperparameters from tuning\n",
        "\n",
        "    def load_and_preprocess_data(self, file_path: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Load and preprocess the labeled dataset with enhanced handling for NOT_M&A cases.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the CSV file containing labeled data\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: Processed data in Spacy-compatible format\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(file_path, encoding='utf-8')\n",
        "        print(f\"Loaded {len(df)} rows from {file_path}\")\n",
        "        # Data validation\n",
        "        required_cols = ['headline', 'entity_name', 'start', 'end', 'is_company', 'M&A_label']\n",
        "        assert all(col in df.columns for col in required_cols), \"CSV file must contain required columns\"\n",
        "\n",
        "        # Convert M&A_label to our entity types with NOT_M&A handling\n",
        "        def map_label(label):\n",
        "            if label == 'Acquirer':\n",
        "                return 'ACQUIRER'\n",
        "            elif label == 'Target':\n",
        "                return 'TARGET'\n",
        "            elif label == 'Seller':\n",
        "                return 'SELLER'\n",
        "            elif label == 'not_M&A':\n",
        "                return 'NOT_M&A'\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        df['entity_type'] = df['M&A_label'].apply(map_label)\n",
        "\n",
        "        # Group by headline to create Spacy-compatible format\n",
        "        grouped = df.groupby('headline').apply(\n",
        "            lambda x: {\n",
        "                'text': x.name,\n",
        "                'entities': [(row.start, row.end, row.entity_type)\n",
        "                           for row in x.itertuples() if pd.notna(row.entity_type)]\n",
        "            }\n",
        "        ).reset_index(name='spacy_format')\n",
        "\n",
        "        return grouped['spacy_format'].tolist()\n",
        "\n",
        "    def convert_to_spacy_format(self, data: List[Dict], output_file: str) -> None:\n",
        "        \"\"\"\n",
        "        Convert data to Spacy's binary format with enhanced NOT_M&A handling.\n",
        "\n",
        "        Args:\n",
        "            data (List[Dict]): List of training examples\n",
        "            output_file (str): Path to save the Spacy binary file\n",
        "        \"\"\"\n",
        "        doc_bin = DocBin()\n",
        "\n",
        "        for example in data:\n",
        "            doc = self.nlp.make_doc(example['text'])\n",
        "            entities = []\n",
        "\n",
        "            # Handle cases where there are no entities (NOT_M&A)\n",
        "            if not example['entities']:\n",
        "                # For NOT_M&A, we don't add any entities\n",
        "                pass\n",
        "            else:\n",
        "                for start, end, label in example['entities']:\n",
        "                    span = doc.char_span(start, end, label=label)\n",
        "                    if span is not None:\n",
        "                        entities.append(span)\n",
        "\n",
        "            filtered_entities = filter_spans(entities)\n",
        "            doc.ents = filtered_entities\n",
        "            doc_bin.add(doc)\n",
        "\n",
        "        doc_bin.to_disk(output_file)\n",
        "        print(f\"Saved processed data to {output_file}\")\n",
        "\n",
        "    def objective(self, trial: optuna.Trial, train_data: str, validation: str) -> float:\n",
        "        \"\"\"\n",
        "        Objective function for Optuna hyperparameter optimization.\n",
        "\n",
        "        Args:\n",
        "            trial (optuna.Trial): Optuna trial object\n",
        "            train_data (str): Path to training data in Spacy binary format\n",
        "            validation (str): Path to val data in Spacy binary format\n",
        "\n",
        "        Returns:\n",
        "            float: F1 score to maximize\n",
        "        \"\"\"\n",
        "        # Suggest hyperparameters\n",
        "        dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
        "        batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 24, 32])\n",
        "        learn_rate = trial.suggest_float(\"learn_rate\", 1e-4, 1e-3, log=True)\n",
        "        n_iter = trial.suggest_int(\"n_iter\", 20, 50)\n",
        "\n",
        "        # Create a fresh model for this trial\n",
        "        trial_nlp = spacy.blank(\"en\")\n",
        "\n",
        "        # Add NER pipeline\n",
        "        if \"ner\" not in trial_nlp.pipe_names:\n",
        "            ner = trial_nlp.add_pipe(\"ner\")\n",
        "        else:\n",
        "            ner = trial_nlp.get_pipe(\"ner\")\n",
        "\n",
        "        # Add entity labels\n",
        "        for label in self.entity_types:\n",
        "            ner.add_label(label)\n",
        "\n",
        "        # Load data\n",
        "        train_docbin = DocBin().from_disk(train_data)\n",
        "        val_docbin = DocBin().from_disk(validation)\n",
        "\n",
        "        train_docs = list(train_docbin.get_docs(trial_nlp.vocab))\n",
        "        val_docs = list(val_docbin.get_docs(trial_nlp.vocab))\n",
        "        # Convert to Example objects\n",
        "        train_examples = []\n",
        "        for doc in train_docs:\n",
        "            predoc = trial_nlp.make_doc(doc.text)\n",
        "            example = spacy.training.Example(predoc, doc)\n",
        "            train_examples.append(example)\n",
        "        # Disable other pipes and train\n",
        "        other_pipes = [pipe for pipe in trial_nlp.pipe_names if pipe != \"ner\"]\n",
        "        with trial_nlp.disable_pipes(*other_pipes):\n",
        "            optimizer = trial_nlp.initialize(lambda: train_examples)\n",
        "            optimizer.learn_rate = learn_rate\n",
        "\n",
        "            # Train with suggested parameters\n",
        "            for itn in range(n_iter):\n",
        "                random.shuffle(train_examples)\n",
        "                losses = {}\n",
        "\n",
        "                batches = spacy.util.minibatch(train_examples, size=batch_size)\n",
        "                for batch in batches:\n",
        "                    trial_nlp.update(\n",
        "                        batch,\n",
        "                        drop=dropout,\n",
        "                        losses=losses,\n",
        "                        sgd=optimizer\n",
        "                    )\n",
        "\n",
        "        # Evaluate on val set\n",
        "        val_metrics = self._evaluate_with_confusion_matrix(trial_nlp, val_docs)\n",
        "        f1_score = val_metrics['weighted avg']['f1-score']\n",
        "\n",
        "        return f1_score\n",
        "\n",
        "    def tune_hyperparameters(self, train_data: str, validation: str, n_trials: int = 25) -> Dict:\n",
        "        \"\"\"\n",
        "        Perform hyperparameter tuning using Optuna.\n",
        "\n",
        "        Args:\n",
        "            train_data (str): Path to training data\n",
        "            validation (str): Path to val data\n",
        "            n_trials (int): Number of optimization trials\n",
        "\n",
        "        Returns:\n",
        "            Dict: Best hyperparameters found\n",
        "        \"\"\"\n",
        "        sampler = TPESampler(seed=42)  # For reproducible results\n",
        "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "        study.optimize(lambda trial: self.objective(trial, train_data, validation), n_trials=n_trials)\n",
        "\n",
        "        self.best_params = study.best_params\n",
        "        print(f\"Best hyperparameters: {self.best_params}\")\n",
        "        print(f\"Best F1 score: {study.best_value:.4f}\")\n",
        "\n",
        "        return self.best_params\n",
        "\n",
        "    def train_model(self, train_data: str, validation: str, output_dir: Path,\n",
        "                   use_tuned_params: bool = True, n_iter: Optional[int] = None) -> None:\n",
        "        \"\"\"\n",
        "        Train the Spacy NER model with optional hyperparameter tuning.\n",
        "\n",
        "        Args:\n",
        "            train_data (str): Path to training data\n",
        "            validation (str): Path to val data\n",
        "            output_dir (Path): Directory to save the trained model\n",
        "            use_tuned_params (bool): Whether to use tuned hyperparameters\n",
        "            n_iter (int, optional): Override for number of iterations\n",
        "        \"\"\"\n",
        "        # Create the NER pipeline\n",
        "        if \"ner\" not in self.nlp.pipe_names:\n",
        "            ner = self.nlp.add_pipe(\"ner\")\n",
        "        else:\n",
        "            ner = self.nlp.get_pipe(\"ner\")\n",
        "\n",
        "        # Add entity labels\n",
        "        for label in self.entity_types:\n",
        "            ner.add_label(label)\n",
        "\n",
        "        # Load data\n",
        "        train_docbin = DocBin().from_disk(train_data)\n",
        "        val_docbin = DocBin().from_disk(validation)\n",
        "\n",
        "        train_docs = list(train_docbin.get_docs(self.nlp.vocab))\n",
        "        val_docs = list(val_docbin.get_docs(self.nlp.vocab))\n",
        "\n",
        "        # Convert to Example objects\n",
        "        train_examples = []\n",
        "        for doc in train_docs:\n",
        "            predoc = self.nlp.make_doc(doc.text)\n",
        "            example = spacy.training.Example(predoc, doc)\n",
        "            train_examples.append(example)\n",
        "\n",
        "        # Set training parameters\n",
        "        if use_tuned_params and self.best_params:\n",
        "            dropout = self.best_params.get(\"dropout\", 0.3)\n",
        "            batch_size = self.best_params.get(\"batch_size\", 8)\n",
        "            learn_rate = self.best_params.get(\"learn_rate\", 0.001)\n",
        "            n_iter = self.best_params.get(\"n_iter\", 30) if n_iter is None else n_iter\n",
        "        else:\n",
        "            dropout = 0.3\n",
        "            batch_size = 8\n",
        "            learn_rate = 0.001\n",
        "            n_iter = 30 if n_iter is None else n_iter\n",
        "\n",
        "        # Disable other pipes and train\n",
        "        other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != \"ner\"]\n",
        "        with self.nlp.disable_pipes(*other_pipes):\n",
        "            optimizer = self.nlp.initialize(lambda: train_examples)\n",
        "            optimizer.learn_rate = learn_rate\n",
        "\n",
        "            best_f1 = 0\n",
        "            no_improvement = 0\n",
        "            patience = 5  # Early stopping patience\n",
        "\n",
        "            print(\"Beginning training with parameters:\")\n",
        "            print(f\"Dropout: {dropout}, Batch size: {batch_size}, Learn rate: {learn_rate}, Iterations: {n_iter}\")\n",
        "\n",
        "            patience = 3\n",
        "            no_improvement = 0\n",
        "            best_f1 = 0\n",
        "\n",
        "            for itn in range(n_iter):\n",
        "                random.shuffle(train_examples)\n",
        "                losses = {}\n",
        "\n",
        "                batches = spacy.util.minibatch(train_examples, size=batch_size)\n",
        "                for batch in batches:\n",
        "                    self.nlp.update(\n",
        "                        batch,\n",
        "                        drop=dropout,\n",
        "                        losses=losses,\n",
        "                        sgd=optimizer\n",
        "                    )\n",
        "\n",
        "                # Evaluate on val set every 5 iterations to save time\n",
        "                if itn % 5 == 0:\n",
        "                    val_metrics = self._evaluate_with_confusion_matrix(self.nlp, val_docs)\n",
        "                    current_f1 = val_metrics['weighted avg']['f1-score']\n",
        "\n",
        "                    if current_f1 > best_f1:\n",
        "                        best_f1 = current_f1\n",
        "                        no_improvement = 0\n",
        "                    else:\n",
        "                        no_improvement += 1\n",
        "\n",
        "                    if no_improvement >= patience:\n",
        "                        print(f\"Early stopping triggered at iteration {itn}.\")\n",
        "                        break  # Early stop this trial\n",
        "\n",
        "        # Load the best model\n",
        "        self.nlp = spacy.load(output_dir / \"best_model\")\n",
        "        print(\"Training complete. Best model loaded.\")\n",
        "\n",
        "    def _evaluate_with_confusion_matrix(self, nlp_model, docs: List) -> Dict:\n",
        "        \"\"\"\n",
        "        Enhanced evaluation with multilabel confusion matrix.\n",
        "\n",
        "        Args:\n",
        "            nlp_model: Spacy model to evaluate\n",
        "            docs (List): List of Spacy Doc objects to evaluate on\n",
        "\n",
        "        Returns:\n",
        "            Dict: Evaluation metrics including confusion matrices\n",
        "        \"\"\"\n",
        "        true_ents = []\n",
        "        pred_ents = []\n",
        "\n",
        "        for doc in docs:\n",
        "            true_ents.extend([(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents])\n",
        "\n",
        "            # Predict entities\n",
        "            pred_doc = nlp_model(doc.text)\n",
        "            pred_ents.extend([(ent.start_char, ent.end_char, ent.label_) for ent in pred_doc.ents])\n",
        "\n",
        "        # Convert to classification format\n",
        "        y_true = [ent[2] for ent in true_ents]\n",
        "        y_pred = []\n",
        "\n",
        "        # Match predicted entities to true entities\n",
        "        for true_ent in true_ents:\n",
        "            matched = False\n",
        "            for pred_ent in pred_ents:\n",
        "                if pred_ent[0] == true_ent[0] and pred_ent[1] == true_ent[1]:\n",
        "                    y_pred.append(pred_ent[2])\n",
        "                    matched = True\n",
        "                    break\n",
        "            if not matched:\n",
        "                y_pred.append('O')  # No entity predicted\n",
        "\n",
        "        # Generate classification report\n",
        "        report = classification_report(\n",
        "            y_true, y_pred,\n",
        "            labels=self.entity_types,\n",
        "            target_names=self.entity_types,\n",
        "            output_dict=True\n",
        "        )\n",
        "\n",
        "        # Generate multilabel confusion matrix\n",
        "        label_map = {label: i for i, label in enumerate(self.entity_types)}\n",
        "        y_true_bin = np.zeros((len(y_true), len(self.entity_types)))\n",
        "        y_pred_bin = np.zeros((len(y_pred), len(self.entity_types)))\n",
        "\n",
        "        for i, (true, pred) in enumerate(zip(y_true, y_pred)):\n",
        "            if true in label_map:\n",
        "                y_true_bin[i, label_map[true]] = 1\n",
        "            if pred in label_map:\n",
        "                y_pred_bin[i, label_map[pred]] = 1\n",
        "\n",
        "        confusion_matrices = multilabel_confusion_matrix(y_true_bin, y_pred_bin)\n",
        "        report['confusion_matrices'] = {\n",
        "            label: matrix for label, matrix in zip(self.entity_types, confusion_matrices)\n",
        "        }\n",
        "\n",
        "        return report\n",
        "\n",
        "    def evaluate_model(self, test_data: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate the model on test data with comprehensive metrics.\n",
        "\n",
        "        Args:\n",
        "            test_data (str): Path to test data in Spacy binary format\n",
        "\n",
        "        Returns:\n",
        "            Dict: Evaluation metrics including confusion matrices\n",
        "        \"\"\"\n",
        "        test_docbin = DocBin().from_disk(test_data)\n",
        "        test_docs = list(test_docbin.get_docs(self.nlp.vocab))\n",
        "        metrics = self._evaluate_with_confusion_matrix(self.nlp, test_docs)\n",
        "\n",
        "        print(\"\\nTest Set Evaluation:\")\n",
        "        for label in self.entity_types:\n",
        "            if label in metrics:\n",
        "                print(f\"{label} - Precision: {metrics[label]['precision']:.4f}, \"\n",
        "                      f\"Recall: {metrics[label]['recall']:.4f}, \"\n",
        "                      f\"F1: {metrics[label]['f1-score']:.4f}\")\n",
        "\n",
        "        print(f\"\\nMacro Avg F1: {metrics['macro avg']['f1-score']:.4f}\")\n",
        "        print(f\"Weighted Avg F1: {metrics['weighted avg']['f1-score']:.4f}\")\n",
        "\n",
        "        # Print confusion matrices\n",
        "        print(\"\\nConfusion Matrices (TN, FP, FN, TP):\")\n",
        "        for label in self.entity_types:\n",
        "            print(f\"\\n{label}:\")\n",
        "            print(metrics['confusion_matrices'][label])\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def predict_entities(self, text: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Predict entities in a given text with enhanced NOT_M&A handling.\n",
        "\n",
        "        Args:\n",
        "            text (str): Text to predict entities in\n",
        "\n",
        "        Returns:\n",
        "            Dict: Dictionary containing entities by type\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        result = {\n",
        "            'acquirers': [],\n",
        "            'targets': [],\n",
        "            'sellers': [],\n",
        "            'not_ma': True if not doc.ents else False,  # True if no entities found\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == 'ACQUIRER':\n",
        "                result['acquirers'].append(ent.text)\n",
        "                result['not_ma'] = False\n",
        "            elif ent.label_ == 'TARGET':\n",
        "                result['targets'].append(ent.text)\n",
        "                result['not_ma'] = False\n",
        "            elif ent.label_ == 'SELLER':\n",
        "                result['sellers'].append(ent.text)\n",
        "                result['not_ma'] = False\n",
        "\n",
        "        return result\n",
        "\n",
        "    def predict_on_unlabeled_data(self, input_file: str, output_file: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Predict entities on unlabeled data and save results with NOT_M&A flag.\n",
        "\n",
        "        Args:\n",
        "            input_file (str): Path to CSV with unlabeled headlines\n",
        "            output_file (str): Path to save predictions\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with predictions\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(input_file)\n",
        "        assert 'headline' in df.columns, \"Input CSV must contain 'headline' column\"\n",
        "\n",
        "        predictions = []\n",
        "        for text in df['headline']:\n",
        "            pred = self.predict_entities(text)\n",
        "            predictions.append({\n",
        "                'headline': text,\n",
        "                'acquirers': ', '.join(pred['acquirers']),\n",
        "                'targets': ', '.join(pred['targets']),\n",
        "                'sellers': ', '.join(pred['sellers']),\n",
        "                'is_not_ma': pred['not_ma']\n",
        "            })\n",
        "\n",
        "        pred_df = pd.DataFrame(predictions)\n",
        "        pred_df.to_csv(output_file, index=False)\n",
        "        print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "        return pred_df"
      ],
      "metadata": {
        "id": "Xra28oA50ffq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Configuration\n",
        "    LABELED_DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/MA_NER_Spacy/ner_annotations_5k.csv\"\n",
        "    UNLABELED_DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/MA_NER_Spacy/unseen_headlines.csv\"\n",
        "    OUTPUT_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks/MA_NER_Spacy/\")\n",
        "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "    # Initialize enhanced model\n",
        "    ner_model = EnhancedSpacyNERModel()\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    data = ner_model.load_and_preprocess_data(LABELED_DATA_PATH)\n",
        "    print(\"length of preprocessed data:\", len(data))\n",
        "    # Split data into train, val, test (70/15/15)\n",
        "    train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n",
        "    validation, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "    train_data.to_csv(\"/content/drive/MyDrive/Colab Notebooks/MA_NER_Spacy/train_data.csv\", index=False)\n",
        "    validation.to_csv(\"/content/drive/MyDrive/Colab Notebooks/MA_NER_Spacy/validation.csv\", index=False)\n",
        "    test_data.to_csv(\"/content/drive/MyDrive/Colab Notebooks/MA_NER_Spacy/test_data.csv\", index=False)\n",
        "\n",
        "    print(f\"Data split: {len(train_data)} train, {len(validation)} val, {len(test_data)} test\")\n",
        "\n",
        "    # Convert to Spacy format and save\n",
        "    train_file = OUTPUT_DIR / \"train.spacy\"\n",
        "    val_file = OUTPUT_DIR / \"val.spacy\"\n",
        "    test_file = OUTPUT_DIR / \"test.spacy\"\n",
        "\n",
        "    ner_model.convert_to_spacy_format(train_data, train_file)\n",
        "    ner_model.convert_to_spacy_format(validation, val_file)\n",
        "    ner_model.convert_to_spacy_format(test_data, test_file)\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    print(\"\\nStarting hyperparameter tuning...\")\n",
        "    best_params = ner_model.tune_hyperparameters(train_file, val_file, n_trials=25)\n",
        "\n",
        "    # Train the model with best parameters\n",
        "    print(\"\\nTraining model with best parameters...\")\n",
        "    ner_model.train_model(train_file, val_file, OUTPUT_DIR, use_tuned_params=True)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    test_metrics = ner_model.evaluate_model(test_file)\n",
        "\n",
        "    # Predict on unlabeled data\n",
        "    print(\"\\nPredicting on unlabeled data...\")\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    prediction_file = OUTPUT_DIR / f\"predictions_{timestamp}.csv\"\n",
        "    ner_model.predict_on_unlabeled_data(UNLABELED_DATA_PATH, prediction_file)"
      ],
      "metadata": {
        "id": "x3t1Xkpublpq"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hREkrj6hbl1z",
        "outputId": "27ac4526-3856-422d-8ab6-7610f2871de6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Loaded 5136 rows from /content/drive/MyDrive/Colab Notebooks/MA_NER_Spacy/ner_annotations_5k.csv\n",
            "length of preprocessed data: 3545\n",
            "Data split: 2481 train, 532 dev, 532 test\n",
            "Saved processed data to /content/drive/MyDrive/Colab Notebooks/MA_NER_Spacy/train.spacy\n",
            "Saved processed data to /content/drive/MyDrive/Colab Notebooks/MA_NER_Spacy/dev.spacy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-12 10:26:21,704] A new study created in memory with name: no-name-0089b1ef-b954-4d2c-845b-9a7dbc1815de\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved processed data to /content/drive/MyDrive/Colab Notebooks/MA_NER_Spacy/test.spacy\n",
            "\n",
            "Starting hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-12 10:29:20,525] Trial 0 finished with value: 0.5568640256377575 and parameters: {'dropout': 0.3123620356542087, 'batch_size': 8, 'learn_rate': 0.00014321698289111514, 'n_iter': 21}. Best is trial 0 with value: 0.5568640256377575.\n",
            "[I 2025-06-12 10:31:23,775] Trial 1 finished with value: 0.5701065131047425 and parameters: {'dropout': 0.45985284373248053, 'batch_size': 32, 'learn_rate': 0.0006798962421591129, 'n_iter': 26}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 10:34:47,491] Trial 2 finished with value: 0.5679551649143766 and parameters: {'dropout': 0.2545474901621302, 'batch_size': 24, 'learn_rate': 0.00019553708662745247, 'n_iter': 38}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 10:37:36,995] Trial 3 finished with value: 0.5442544235772612 and parameters: {'dropout': 0.24184815819561256, 'batch_size': 32, 'learn_rate': 0.0001583703155911876, 'n_iter': 35}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 10:42:37,804] Trial 4 finished with value: 0.5629497110449196 and parameters: {'dropout': 0.37772437065861275, 'batch_size': 16, 'learn_rate': 0.000888966790701893, 'n_iter': 49}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 10:45:47,743] Trial 5 finished with value: 0.5142675854657435 and parameters: {'dropout': 0.4425192044349383, 'batch_size': 24, 'learn_rate': 0.00013244581340099356, 'n_iter': 35}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 10:50:51,739] Trial 6 finished with value: 0.5568863401502055 and parameters: {'dropout': 0.21031655633456553, 'batch_size': 8, 'learn_rate': 0.000331182988807238, 'n_iter': 36}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 10:57:28,610] Trial 7 finished with value: 0.5397602377171697 and parameters: {'dropout': 0.2554563366576581, 'batch_size': 8, 'learn_rate': 0.00039618677904065835, 'n_iter': 48}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 11:01:03,577] Trial 8 finished with value: 0.5496048368396677 and parameters: {'dropout': 0.22654775061557586, 'batch_size': 32, 'learn_rate': 0.000186788025710707, 'n_iter': 45}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 11:05:05,824] Trial 9 finished with value: 0.563813904938832 and parameters: {'dropout': 0.3070259980080768, 'batch_size': 32, 'learn_rate': 0.00011872731425335906, 'n_iter': 50}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 11:07:22,452] Trial 10 finished with value: 0.5576171443206943 and parameters: {'dropout': 0.49365352981280786, 'batch_size': 16, 'learn_rate': 0.0008614663008790045, 'n_iter': 22}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 11:09:53,909] Trial 11 finished with value: 0.5538723117439454 and parameters: {'dropout': 0.39202655074026693, 'batch_size': 24, 'learn_rate': 0.0005548947154585526, 'n_iter': 28}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 11:13:29,819] Trial 12 finished with value: 0.5520951681990643 and parameters: {'dropout': 0.4908162923972057, 'batch_size': 24, 'learn_rate': 0.00024120158447883412, 'n_iter': 41}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 11:16:02,502] Trial 13 finished with value: 0.5598716641594244 and parameters: {'dropout': 0.2937034618443587, 'batch_size': 24, 'learn_rate': 0.0005293970390012881, 'n_iter': 29}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 11:18:21,577] Trial 14 finished with value: 0.5380756233033419 and parameters: {'dropout': 0.40696712700490756, 'batch_size': 32, 'learn_rate': 0.00023995745897879495, 'n_iter': 29}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 11:21:54,422] Trial 15 finished with value: 0.5476018515283805 and parameters: {'dropout': 0.3529000232006496, 'batch_size': 24, 'learn_rate': 0.0006003484492723333, 'n_iter': 40}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 11:23:49,066] Trial 16 finished with value: 0.5177884320046544 and parameters: {'dropout': 0.4434356010732394, 'batch_size': 32, 'learn_rate': 0.00023278287397370608, 'n_iter': 24}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 11:27:02,155] Trial 17 finished with value: 0.5317812471616589 and parameters: {'dropout': 0.27316248825907047, 'batch_size': 16, 'learn_rate': 0.0001014238997200542, 'n_iter': 32}. Best is trial 1 with value: 0.5701065131047425.\n",
            "[I 2025-06-12 11:30:15,601] Trial 18 finished with value: 0.5896400897600578 and parameters: {'dropout': 0.349068998923672, 'batch_size': 32, 'learn_rate': 0.00042288835492685126, 'n_iter': 40}. Best is trial 18 with value: 0.5896400897600578.\n",
            "[I 2025-06-12 11:33:53,014] Trial 19 finished with value: 0.5648674940224416 and parameters: {'dropout': 0.3409928911062702, 'batch_size': 32, 'learn_rate': 0.0007147028012869901, 'n_iter': 44}. Best is trial 18 with value: 0.5896400897600578.\n",
            "[I 2025-06-12 11:36:04,807] Trial 20 finished with value: 0.554707358574458 and parameters: {'dropout': 0.43331348105208234, 'batch_size': 32, 'learn_rate': 0.00042883402406646237, 'n_iter': 25}. Best is trial 18 with value: 0.5896400897600578.\n",
            "[I 2025-06-12 11:39:28,062] Trial 21 finished with value: 0.566987535885485 and parameters: {'dropout': 0.34169658850016044, 'batch_size': 32, 'learn_rate': 0.00031193921901734117, 'n_iter': 39}. Best is trial 18 with value: 0.5896400897600578.\n",
            "[I 2025-06-12 11:43:04,938] Trial 22 finished with value: 0.562280885377441 and parameters: {'dropout': 0.28030875634943475, 'batch_size': 24, 'learn_rate': 0.00043464842429496927, 'n_iter': 38}. Best is trial 18 with value: 0.5896400897600578.\n",
            "[I 2025-06-12 11:46:42,622] Trial 23 finished with value: 0.5498110577880817 and parameters: {'dropout': 0.3623363812015695, 'batch_size': 32, 'learn_rate': 0.000702640287078392, 'n_iter': 42}. Best is trial 18 with value: 0.5896400897600578.\n",
            "[I 2025-06-12 11:49:31,994] Trial 24 finished with value: 0.5426370146166233 and parameters: {'dropout': 0.41408034874173644, 'batch_size': 32, 'learn_rate': 0.00030314916068571024, 'n_iter': 34}. Best is trial 18 with value: 0.5896400897600578.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'dropout': 0.349068998923672, 'batch_size': 32, 'learn_rate': 0.00042288835492685126, 'n_iter': 40}\n",
            "Best F1 score: 0.5896\n",
            "\n",
            "Training model with best parameters...\n",
            "Beginning training with parameters:\n",
            "Dropout: 0.349068998923672, Batch size: 32, Learn rate: 0.00042288835492685126, Iterations: 40\n",
            "Training complete. Best model loaded.\n",
            "\n",
            "Evaluating on test set...\n",
            "\n",
            "Test Set Evaluation:\n",
            "ACQUIRER - Precision: 0.5406, Recall: 0.8893, F1: 0.6724\n",
            "TARGET - Precision: 0.8089, Recall: 0.5826, F1: 0.6773\n",
            "SELLER - Precision: 1.0000, Recall: 0.0222, F1: 0.0435\n",
            "NOT_M&A - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "\n",
            "Macro Avg F1: 0.3483\n",
            "Weighted Avg F1: 0.4433\n",
            "\n",
            "Confusion Matrices (TN, FP, FN, TP):\n",
            "\n",
            "ACQUIRER:\n",
            "[[275 198]\n",
            " [ 29 233]]\n",
            "\n",
            "TARGET:\n",
            "[[487  30]\n",
            " [ 91 127]]\n",
            "\n",
            "SELLER:\n",
            "[[690   0]\n",
            " [ 44   1]]\n",
            "\n",
            "NOT_M&A:\n",
            "[[525   0]\n",
            " [210   0]]\n",
            "\n",
            "Predicting on unlabeled data...\n",
            "Predictions saved to /content/drive/MyDrive/Colab Notebooks/MA_NER_Spacy/predictions_20250612_115253.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JXONDEnMbwOb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AV8aorpsp2Q-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}